{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "x = data[\"data\"]#[:, :3]\n",
    "\n",
    "print(x)\n",
    "\n",
    "#y = np.where(data[\"target\"] >= 1, 1, 0)\n",
    "y = np.array(data[\"target\"])#.reshape(1, -1)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.fc1 = nn.Linear(4, 8)\n",
    "        #self.fc2 = nn.Linear(8, 4)\n",
    "        #self.fc3 = nn.Linear(4, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 16)  # 4 inputs to a hidden layer with 16 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 3)  # 16 hidden to 3 output neurons (classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        self.to()\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    y = F.relu(self.fc1(x))\n",
    "    #    y = F.relu(self.fc2(y))\n",
    "    #    y = F.softmax(self.fc3(y), dim=1)\n",
    "    #    return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def my_train(self, x_train, y_train):\n",
    "        # Initialize loss function and optimizer with model parameters and learning rate.\n",
    "        #loss = nn.MSELoss()\n",
    "        # For binary values:\n",
    "        # loss = nn.BCELoss()\n",
    "        #loss = nn.NLLLoss() ## Do be found\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=0.009)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "        # Initialize error list for later visualization.\n",
    "        self.epoch_errors = []\n",
    "\n",
    "        # Train the model in n epochs.\n",
    "        for epoch in range(500):\n",
    "            # Set all gradients to zero since it is a new iteration and optimization round.\n",
    "            optimizer.zero_grad()\n",
    "            # Compute error on training data.\n",
    "            # Do some transformations on the data to get it in the right tensor shape for the model.\n",
    "            error = loss(self(torch.from_numpy(x_train).type(\n",
    "                torch.FloatTensor)), torch.from_numpy(y_train).unsqueeze(1).type(torch.FloatTensor))\n",
    "            # Store error for later visualization.\n",
    "            self.epoch_errors.append(error.item())\n",
    "            # Compute gradients for all the parameters.\n",
    "            error.backward()\n",
    "            # Update all parameters using the gradients and optimizer formula.\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m IrisNN()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmy_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36mIrisNN.my_train\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Compute error on training data.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Do some transformations on the data to get it in the right tensor shape for the model.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Store error for later visualization.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_errors\u001b[38;5;241m.\u001b[39mappend(error\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Projects/ml-tutorial/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-tutorial/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/ml-tutorial/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ml-tutorial/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = IrisNN()\n",
    "model.my_train(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot error evolution over the epochs.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(model.epoch_errors, color=\"#1ACC94\")\n",
    "plt.title('Loss Entwicklung ber Epochen')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# predict a single value\n",
    "#foo = np.array([4.9, 3.0 , 1.4, 0.2])\n",
    "foo = np.array([5.9, 3.0 , 5.1, 1.8])\n",
    "y_pred = model(torch.from_numpy(foo).type(\n",
    "    torch.FloatTensor)).detach().numpy()\n",
    "\n",
    "print(y_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
